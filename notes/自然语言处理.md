精度计算

1.未识别的实体

2.识别出来但是错误的实体

3.计算两个实体是否匹配

4.两个类型的实体


自然语言处理（NLP）中的词向量是一项重要的技术，它有助于将文本数据映射到连续向量空间，以便计算机能够更好地理解和处理自然语言。以下是与NLP词向量相关的全部知识要点：

1. **词向量（Word Vectors）**:
   - 词向量是一种将单词映射到向量空间中的表示形式。每个单词都被表示为一个实数向量，以便计算机可以理解和处理文本。
2. **分布式表示（Distributed Representation）**:
   - 词向量通常是分布式表示的一部分，其中每个单词的含义由其周围的上下文单词决定。这有助于捕捉语义信息。
3. **Word2Vec**:
   - Word2Vec是一种流行的词向量学习方法，它有两种变种：Skip-gram和CBOW。这些模型通过预测上下文单词来学习词向量。
4. **GloVe**:
   - GloVe（Global Vectors for Word Representation）是另一种词向量学习算法，它使用了全局统计信息来学习单词之间的关系。
5. **FastText**:
   - FastText是一种基于子词级别的词向量学习方法，它能够捕捉词汇的内部结构和复杂性。
6. **预训练词向量**:
   - 预训练词向量是在大型语料库上训练得到的词向量，可以用于各种NLP任务，如情感分析、文本分类、实体识别等。
7. **词向量应用**:
   - 词向量在NLP任务中有广泛的应用，包括机器翻译、文本生成、情感分析、问题回答、搜索引擎、信息检索等。
8. **可视化和相似度**:
   - 词向量可以用于可视化文本数据，例如通过t-SNE降维。它们还可用于测量词汇之间的相似度，如余弦相似度。
9. **语义空间和语法空间**:
   - 词向量中的语义空间捕捉了单词的语义关系，而语法空间则捕捉了单词的语法关系。
10. **上下文窗口**:
    - 在词向量学习中，通常使用上下文窗口来确定每个单词周围的上下文词汇。
11. **负采样和负采样损失**:
    - 负采样是一种Word2Vec的改进技术，它有助于提高训练效率。负采样损失用于衡量模型的性能。
12. **词汇表（Vocabulary）**:
    - 词向量模型通常在有限的词汇表上进行训练，其中包括训练数据中的单词。
13. **迁移学习**:
    - 预训练的词向量可以用于迁移学习，将知识从一个NLP任务转移到另一个任务，从而提高性能。

### Word2Vec的工作原理

Word2Vec模型的核心思想是通过观察单词在上下文中的共现关系来学习单词的分布式表示。它有两主要变种：Skip-gram和CBOW。

### Skip-gram模型

1. **目标**:
   - Skip-gram的目标是从中心词汇预测其周围的上下文词汇。
2. **条件概率**:
   - Skip-gram模型的条件概率公式如下: $P(O=o \mid C=c) = \sigma(\boldsymbol{u}_o^\top \boldsymbol{v}_c)$
     - 其中，$P(O=o \mid C=c)$ 表示在给定中心词汇$c$的情况下，预测上下文词汇$o$的概率。
     - $\boldsymbol{u}_o$ 表示上下文词汇$o$的词向量。
     - $\boldsymbol{v}_c$ 表示中心词汇$c$的词向量。
     - $\sigma(x)$ 是sigmoid函数，将$x$映射到(0,1)之间的概率。
3. **损失函数**:
   - Skip-gram模型使用负对数似然（Negative Log Likelihood）作为损失函数，如下所示: $J_{\text{skip-gram}}(\theta) = -\log P(O=o \mid C=c)$

### CBOW模型:

1. **目标**:
   - CBOW的目标是从上下文词汇预测中心词汇。
2. **条件概率**:
   - CBOW模型的条件概率公式如下: $P(O=o \mid C=c_1, c_2, ..., c_n) = \sigma(\boldsymbol{u}o^\top (\boldsymbol{v}{c_1} + \boldsymbol{v}{c_2} + ... + \boldsymbol{v}{c_n}))$
     - 其中，$P(O=o \mid C=c_1, c_2, ..., c_n)$ 表示在给定上下文词汇$c_1, c_2, ..., c_n$的情况下，预测中心词汇$o$的概率。
     - $\boldsymbol{u}_o$ 表示上下文词汇$o$的词向量。
     - $\boldsymbol{v}_{c_i}$ 表示上下文词汇$c_i$的词向量。
     - $\sigma(x)$ 是sigmoid函数。
3. **损失函数**:
   - CBOW模型同样使用负对数似然作为损失函数，如下所示: $J_{\text{CBOW}}(\theta) = -\log P(O=o \mid C=c_1, c_2, ..., c_n)$

这些公式描述了Skip-gram和CBOW模型的数学基础。在训练过程中，目标是最小化这些损失函数，以便学习高质量的词向量，这些向量捕捉了单词之间的语义和语法关系。通常，优化算法（如梯度下降）用于调整词向量以减小损失函数的值，以使模型更好地预测上下文和中心词汇之间的关系。这些学到的词向量可以用于各种自然语言处理任务，如文本分类、情感分析、命名实体识别等。

### Sigmoid函数（也称为逻辑函数）

一种常用的数学函数，它将输入值映射到0和1之间的输出。Sigmoid函数的形状类似于"S"形状的曲线，因此也被称为Sigmoid曲线。这个函数通常用于二元分类问题和逻辑回归等机器学习任务，它将输入值转化为概率值。

Sigmoid函数的数学表达式如下：

$g(z)=\frac{1}{1+e^{-z}}$

![sigmoid函数图片](自然语言处理.assets/sigmoid函数图片.png)

其中：

- $\sigma(x)$ 表示Sigmoid函数的输出。
- $x$ 是输入值。

Sigmoid函数的特点包括：

- 当$x$趋于正无穷大时，$\sigma(x)$趋近于1。
- 当$x$趋于负无穷大时，$\sigma(x)$趋近于0。
- 在$x=0$处，$\sigma(x)$的值为0.5。
- Sigmoid函数是单调递增函数，这意味着随着$x$的增加，$\sigma(x)$的值逐渐增加。

Sigmoid函数的主要应用之一是将线性模型的输出转化为概率值，通常在二元分类问题中使用。例如，在逻辑回归中，线性模型的输出经过Sigmoid函数后，可以表示样本属于某一类别的概率。

Sigmoid函数也用于神经网络中的激活函数，其中它可以帮助神经元的输出处于0和1之间，使网络可以进行非线性映射和学习复杂的函数关系。

这个公式表示了一个关于模型参数 $\theta$ 的似然函数（Likelihood），通常用于在自然语言处理任务中评估语言模型的性能。让我们逐步解释这个公式的各个部分：

- $L(\theta)$：表示似然函数，这是一个关于模型参数 $\theta$ 的函数。似然函数的目标是衡量给定参数 $\theta$ 下观察到的数据的可能性。
- $\prod_{i=1}^{T}$：表示一个乘积符号，它将后面的概率项相乘。在这里，$i$ 是一个从1到$T$的索引，通常表示文本序列中的第$i$个位置。
- $\prod_{_{j\neq0}}$：表示在内部的第二个乘积符号，它将后面的条件概率项相乘。在这里，$j$ 表示相对于中心词汇的上下文词汇的位置，$j \neq 0$ 表示不考虑中心词汇自身。
- $P(w_{t+j}|w_{t};\theta)$：表示在给定模型参数 $\theta$ 下，预测在文本序列中位置 $t+j$ 处的词汇 $w_{t+j}$ 在位置 $t$ 处的中心词汇 $w_t$ 的条件概率。这是一个用于语言建模的概率模型，通常根据上下文预测下一个词汇。

综合起来，这个公式表示了一个关于模型参数 $\theta$ 的似然函数，用于评估模型在给定文本序列下的性能。通过最大化似然函数，可以找到最优的模型参数 $\theta$，使模型能够更好地预测文本序列中的词汇。这在自然语言处理任务中，如语言建模、机器翻译等，非常重要，因为它允许我们训练能够生成自然语言文本的模型。



在Word2Vec模型中，损失函数的目标是最小化，以调整模型的参数以改善词向量的质量。具体而言，Word2Vec中使用的损失函数为Negative Sampling Loss（负采样损失）或Hierarchical Softmax Loss（分层Softmax损失）。以下是Word2Vec损失函数中涉及的参数以及如何更新这些参数的一般方法：



### **Negative Sampling Loss（负采样损失）**：

- **参数**：

  1. $\theta$：模型参数，包括中心词向量和外部词向量。
  2. $K$：负样本数，即从词汇表中随机采样的负样本数。

- **损失函数**： Negative Sampling Loss的目标是最小化该损失函数。具体损失函数如下：

  $J_{\text{neg-sampling}} ( \theta ) = - \log P ( O = 1 \mid C = c ) - \sum _ { k = 1 }^{K}\log P(O=0\mid C=c,w_k)$

- **更新参数的方法**：

  - 通常使用优化算法（如梯度下降）来最小化上述损失函数。损失函数的梯度告诉我们如何调整模型参数以减小损失。
  - 对于每个参数，计算其相对于损失函数的梯度。
  - 使用梯度下降或其他优化算法更新模型参数，以减小损失函数的值。

Negative Sampling Loss（负采样损失）是Word2Vec中用于训练词向量的损失函数之一。它包含两个主要参数，以及在损失函数梯度下降中的应用。以下是这两个参数的讲解以及一个简单的示例，说明如何使用梯度下降更新这些参数。

**参数**:

1. $\boldsymbol{v}_c$：中心词汇的词向量
   - 这是一个词汇表中某个中心词汇的词向量表示。$\boldsymbol{v}_c$ 通常是一个固定维度的向量，代表中心词汇的语义。
2. $\boldsymbol{u}_o$：上下文词汇的词向量
   - 这是一个词汇表中某个上下文词汇的词向量表示。$\boldsymbol{u}_o$ 也是一个固定维度的向量，代表上下文词汇的语义。

**梯度下降更新**:

在训练Word2Vec模型时，目标是最小化Negative Sampling Loss，以调整中心词汇的词向量$\boldsymbol{v}_c$和上下文词汇的词向量$\boldsymbol{u}_o$。这是一个优化问题，通常使用梯度下降来更新这些参数，使损失函数最小化。

在梯度下降中，我们计算损失函数关于这两个参数的梯度，然后沿着梯度的负方向更新参数。这是损失函数梯度下降的一般步骤：$J_{\text{neg-sampling}}(\theta)$表示负采样损失，$\theta$表示模型参数，$K$表示负样本数

1. 计算损失函数相对于参数的梯度。对于Negative Sampling Loss，这需要计算以下两个梯度：

   - $\frac{\partial J_{\text{neg-sampling}}}{\partial \boldsymbol{v}_c}$：中心词汇词向量的梯度
   - $\frac{\partial J_{\text{neg-sampling}}}{\partial \boldsymbol{u}_o}$：上下文词汇词向量的梯度

2. 使用计算出的梯度值来更新参数。更新参数的公式通常如下：

   - $\boldsymbol{v}_c = \boldsymbol{v}*c - \alpha \frac{\partial J*{\text{neg-sampling}}}{\partial \boldsymbol{v}_c}$
   - $\boldsymbol{u}_o = \boldsymbol{u}*o - \alpha \frac{\partial J*{\text{neg-sampling}}}{\partial \boldsymbol{u}_o}$

   其中，$\alpha$ 是学习率，控制参数更新的步长。

   在Word2Vec模型中，$J_{\text{neg-sampling}}(\theta)$表示负采样损失，其中：

   - $J_{\text{neg-sampling}}(\theta)$：是Word2Vec模型中使用的负采样损失函数，其目标是最小化该损失函数来学习模型参数。这个损失函数通过优化算法（如梯度下降）来最小化。
   - $\theta$：表示模型的参数，包括中心词汇的词向量和外部词汇的词向量。这些参数是模型训练的目标，通过最小化损失函数来调整它们，以使模型能够更好地捕捉语义关系。
   - $K$：是负样本数，指的是从词汇表中随机采样的负样本词汇的数量。在Word2Vec中，为了训练模型，除了考虑正确的上下文词汇外，还考虑了一定数量的负样本词汇。这些负样本词汇是不应该出现在给定上下文的词汇，模型的目标是将正确的上下文词汇的概率最大化，同时将负样本词汇的概率最小化。

   通过调整模型参数，优化算法尝试最小化$J_{\text{neg-sampling}}(\theta)$，使正样本的概率最大化，负样本的概率最小化，以学习高质量的词向量。$K$通常是一个超参数，可以根据具体的训练任务和词汇表的大小来选择，以平衡训练效率和模型性能。通常情况下，较大的$K$值可能会提高训练稳定性，但会增加计算成本。

**Hierarchical Softmax Loss（分层Softmax损失）**：

- **参数**：

  1. $\theta$：模型参数，包括中心词向量和外部词向量。
  2. 词汇表的树状结构，用于计算条件概率。

- **损失函数**： 分层Softmax Loss的目标是最小化该损失函数。具体损失函数如下：

  $J_{\text{hierarchical-softmax}}(\theta)=-\log P(O=1\mid C=c)$

- **更新参数的方法**：

  - 与负采样损失一样，通常使用优化算法（如梯度下降）来最小化上述损失函数。
  - 计算每个参数相对于损失函数的梯度。
  - 使用梯度下降或其他优化算法来更新模型参数，以减小损失函数的值。

在Word2Vec中，梯度计算和参数更新过程是通过反向传播算法来实现的。这允许模型在训练过程中不断更新参数，以便更好地捕捉单词之间的语义关系。最终，通过迭代训练，Word2Vec学到的词向量可以在各种NLP任务中提供有用的信息。